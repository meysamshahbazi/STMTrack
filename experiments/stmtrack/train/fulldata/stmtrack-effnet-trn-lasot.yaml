test:
  track:
    exp_name: &TEST_NAME "got10k"
    exp_save: &TEST_SAVE "logs/stmtrack-googlenet-fulldata-train-val"
    pipeline:
      name: "STMTrackTracker"
      STMTrackTracker:
        test_lr: 0.95
        window_influence: 0.21
        penalty_k: 0.04
        total_stride: 8
        score_size: 25
        q_size: 200
        m_size: 200
        gpu_memory_threshold: -1
        search_area_factor: 4.0
    tester:
      names: ["GOT10kTester",]
      GOT10kTester:
        exp_name: *TEST_NAME
        exp_save: *TEST_SAVE
        subsets: ["val"]
        data_root: "/media/meysam/ssd/GOT-10k"
        # if set it to be larger than 1, you can't change the evaluation pipeline and hyper-parameters during the training.
        device_num: 1
train:
  track:
    exp_name: &TRAIN_NAME "stmtrack-effnet-lasot-train"
    exp_save: &TRAIN_SAVE "snapshots"
    num_processes: 1
    model:
      use_sync_bn: true
      backbone_m:
        name: "Efficientnet_b0_M"
        Efficientnet_b0_M:
         pretrained: True
      backbone_q:
        name: "Efficientnet_b0_Q"
        Efficientnet_b0_Q:
          pretrained: True
      neck:
        name: "AdjustLayer"
        AdjustLayer:
          in_channels: 40
          out_channels: &OUT_CHANNELS 20
      losses:
        names: [
                "FocalLoss",
                "SigmoidCrossEntropyCenterness",
                "IOULoss",]
        FocalLoss:
          name: "cls"
          weight: 1.0
          alpha: 0.75
          gamma: 2.0
        SigmoidCrossEntropyCenterness:
          name: "ctr"
          weight: 1.0
        IOULoss:
          name: "reg"
          weight: 3.0
      task_head:
        name: "STMHead"
        STMHead:
          total_stride: 8
          score_size: &TRAIN_SCORE_SIZE 25
          q_size: &TRAIN_Q_SIZE 200
          in_channels: *OUT_CHANNELS
      task_model:
        name: "STMTrack"
        STMTrack:
          pretrain_model_path: ""
          amp: &amp False
# ==================================================
    data:
      exp_name: *TRAIN_NAME
      exp_save: *TRAIN_SAVE
      num_epochs: &NUM_EPOCHS 60
      minibatch: &MINIBATCH 32  # 256
      num_workers: 20
      nr_image_per_epoch: &NR_IMAGE_PER_EPOCH 300000
      pin_memory: true
      datapipeline:
        name: "RegularDatapipeline"
      sampler:
        name: "TrackPairSampler"
        TrackPairSampler:
          negative_pair_ratio: 0.33
          num_memory_frames: &NUM_MEMORY_FRAMES 3
        submodules:
          dataset:
            names: [
              "LaSOTDataset",
            ]
            LaSOTDataset:
              ratio: 0.3
              max_diff: 50
              dataset_root: "/media/meysam/ssd/LaSot"
              subset: "train"
              check_integrity: false
          filter:
            name: "TrackPairFilter"
            TrackPairFilter:
              max_area_rate: 0.6
              min_area_rate: 0.001
              max_ratio: 10
      transformer:
        names: ["RandomCropTransformer", ]
        RandomCropTransformer:
          max_scale: 0.3
          max_shift: 0.4
          q_size: *TRAIN_Q_SIZE
          num_memory_frames: *NUM_MEMORY_FRAMES
          search_area_factor: 4.0
      target:
        name: "DenseboxTarget"
        DenseboxTarget:
          total_stride: 8
          score_size: *TRAIN_SCORE_SIZE
          q_size: *TRAIN_Q_SIZE
          num_memory_frames: *NUM_MEMORY_FRAMES
    trainer:
      name: "RegularTrainer"
      RegularTrainer:
        exp_name: *TRAIN_NAME
        exp_save: *TRAIN_SAVE
        max_epoch: *NUM_EPOCHS
        minibatch: *MINIBATCH
        nr_image_per_epoch: *NR_IMAGE_PER_EPOCH
        snapshot: ""
      monitors:
        names: ["TextInfo", "TensorboardLogger"]
        TextInfo:
          {}
        TensorboardLogger:
          exp_name: *TRAIN_NAME
          exp_save: *TRAIN_SAVE

# ==================================================
    optim:
      optimizer:
        name: "Adam"
        Adam:
          # to adjust learning rate, please modify "start_lr" and "end_lr" in lr_policy module bellow
          amp: *amp
          lr: 0.0001
          weight_decay: 0.0001
          minibatch: *MINIBATCH
          nr_image_per_epoch: *NR_IMAGE_PER_EPOCH
      #   name: "SGD"
      #   SGD:
      #     # to adjust learning rate, please modify "start_lr" and "end_lr" in lr_policy module bellow
      #     amp: *amp
      #     momentum: 0.9
      #     weight_decay: 0.0001
      #     minibatch: *MINIBATCH
      #     nr_image_per_epoch: *NR_IMAGE_PER_EPOCH
      #     lr_policy:
      #     - >
      #       {
      #       "name": "LinearLR",
      #       "start_lr": 0.01,
      #       "end_lr": 0.08,
      #       "max_epoch": 1
      #       }
      #     - >
      #       {
      #       "name": "CosineLR",
      #       "start_lr": 0.08,
      #       "end_lr": 0.000001,
      #       "max_epoch": 19
      #       }
      #     lr_multiplier:
      #     - >
      #       {
      #       "name": "backbone",
      #       "regex": "basemodel_.*",
      #       "ratio": 0.1
      #       }
      #     - >
      #       {
      #       "name": "other",
      #       "regex": "^((?!basemodel).)*$",
      #       "ratio": 1
      #       }
      # grad_modifier:
      #   name: "DynamicFreezer"
      #   DynamicFreezer:
      #     schedule:
      #     - >
      #       {
      #       "name": "isConv",
      #       "regex": "basemodel_.*\\.conv\\.",
      #       "epoch": 0,
      #       "freezed": true
      #       }
      #     - >
      #       {
      #       "name": "isConvStage4",
      #       "regex": "basemodel_.*\\.Mixed_6.*\\.conv\\.",
      #       "epoch": 10,
      #       "freezed": false
      #       }
      #     - >
      #       {
      #       "name": "isConvStage3",
      #       "regex": "basemodel_.*\\.Mixed_5.*\\.conv\\.",
      #       "epoch": 10,
      #       "freezed": false
      #       }
